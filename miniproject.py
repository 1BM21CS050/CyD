# -*- coding: utf-8 -*-
"""miniproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/1BM21CS065/ML/blob/main/miniproject.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

df = pd.read_csv(r"/content/labeled_tweets.csv")

df1 = df.copy()
df1=df1.drop_duplicates(subset=['full_text'], keep=False)
df1.head()
df1

df1 = df1.dropna()
df1

from sklearn.model_selection import train_test_split

X = df1['full_text']
y = df1['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X)
print(y)

df1.to_csv('/content/drive/MyDrive/miniproject/miniproject.csv')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve
import seaborn as sns
import matplotlib.pyplot as plt

# Preprocessing and vectorization
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Initialize and train the SVM classifier
svm_clf = SVC(kernel='linear', random_state=42)
svm_clf.fit(X_train_tfidf, y_train)

# Make predictions on the testing set
y_pred_test = svm_clf.predict(X_test_tfidf)

# Evaluate the model on the testing set
print("Classification Report on Testing Set:")
print(classification_report(y_test, y_pred_test))

y_pred_train = svm_clf.predict(X_train_tfidf)

# Evaluate the model on the training set
print("Classification Report on Training Set:")
print(classification_report(y_train, y_pred_train))

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_test)

# Plot confusion matrix heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Cyberbullying', 'Cyberbullying'], yticklabels=['Non-Cyberbullying', 'Cyberbullying'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Convert categorical labels to binary format
y_test_binary = y_test.map({'Non-offensive': 0, 'Offensive': 1})

# ROC curve and AUC-ROC
y_pred_probs = svm_clf.decision_function(X_test_tfidf)
fpr, tpr, _ = roc_curve(y_test_binary, y_pred_probs)
auc_roc = roc_auc_score(y_test_binary, y_pred_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'AUC-ROC = {auc_roc:.2f}')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# Convert categorical labels to binary format
y_test_binary = y_test.map({'Non-offensive': 0, 'Offensive': 1})

# Precision-Recall curve
precision, recall, _ = precision_recall_curve(y_test_binary, y_pred_probs)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='green')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve
import seaborn as sns
import matplotlib.pyplot as plt

tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Initialize and train the Naive Bayes classifier
nb_clf = MultinomialNB()
nb_clf.fit(X_train_tfidf, y_train)

# Make predictions on the testing set
y_pred_test_nb = nb_clf.predict(X_test_tfidf)

# Evaluate the model on the testing set
print("Classification Report on Testing Set:")
print(classification_report(y_test, y_pred_test_nb))

# Evaluate the model on the training set
y_pred_train_nb = nb_clf.predict(X_train_tfidf)
print("Classification Report on Training Set:")
print(classification_report(y_train, y_pred_train_nb))

# Confusion matrix
conf_matrix_nb = confusion_matrix(y_test, y_pred_test_nb)

# Plot confusion matrix heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_nb, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Cyberbullying', 'Cyberbullying'], yticklabels=['Non-Cyberbullying', 'Cyberbullying'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Naive Bayes')
plt.show()

# ROC curve and AUC-ROC
y_pred_probs_nb = nb_clf.predict_proba(X_test_tfidf)[:, 1]
fpr_nb, tpr_nb, _ = roc_curve(y_test_binary, y_pred_probs_nb)
auc_roc_nb = roc_auc_score(y_test_binary, y_pred_probs_nb)

plt.figure(figsize=(8, 6))
plt.plot(fpr_nb, tpr_nb, color='blue', label=f'AUC-ROC = {auc_roc_nb:.2f}')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Naive Bayes')
plt.legend()
plt.show()

# Precision-Recall curve
precision_nb, recall_nb, _ = precision_recall_curve(y_test_binary, y_pred_probs_nb)

plt.figure(figsize=(8, 6))
plt.plot(recall_nb, precision_nb, color='green')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Naive Bayes')
plt.show()

# Calculate accuracy for SVM and Naive Bayes classifiers
accuracy_svm_test = (y_pred_test == y_test).mean()
accuracy_nb_test = (y_pred_test_nb == y_test).mean()

accuracy_svm_train = (y_pred_train == y_train).mean()
accuracy_nb_train = (y_pred_train_nb == y_train).mean()

# Create a DataFrame to display the accuracy comparison
accuracy_df = pd.DataFrame({
    'Method': ['SVM', 'Naive Bayes'],
    'Test Accuracy': [accuracy_svm_test, accuracy_nb_test],
    'Train Accuracy': [accuracy_svm_train, accuracy_nb_train]
})

print("Accuracy Comparison between SVM and Naive Bayes:\n")
print(accuracy_df)

import matplotlib.pyplot as plt

# Create lists for methods and accuracies
methods = ['SVM', 'Naive Bayes']
test_accuracies = [accuracy_svm_test, accuracy_nb_test]
train_accuracies = [accuracy_svm_train, accuracy_nb_train]

# Define colors for test and train accuracies
test_colors = ['blue', 'orange']
train_colors = ['lightblue', 'lightcoral']

# Plotting
plt.figure(figsize=(10, 6))

# Plot bars for test accuracies
plt.bar(methods, test_accuracies, color=test_colors, hatch='/', alpha=0.7, label='Test Accuracy')

# Plot bars for train accuracies
plt.bar(methods, train_accuracies, color=train_colors, hatch='\\', alpha=0.7, label='Train Accuracy')

# Adding labels and title
plt.xlabel('Method')
plt.ylabel('Accuracy')
plt.title('Comparison of Accuracy between SVM and Naive Bayes')
plt.legend()

plt.show()

import numpy as np

# Define width of each bar
bar_width = 0.35

# Define index for each method
index = np.arange(len(methods))

# Plotting
plt.figure(figsize=(10, 6))

# Plot bars for test accuracies
plt.bar(index, test_accuracies, bar_width, color='blue', label='Test Accuracy')

# Plot bars for train accuracies
plt.bar(index + bar_width, train_accuracies, bar_width, color='orange', label='Train Accuracy')

# Adding labels and title
plt.xlabel('Method')
plt.ylabel('Accuracy')
plt.title('Comparison of Accuracy between SVM and Naive Bayes')
plt.xticks(index + bar_width / 2, methods)
plt.legend()

plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Create lists for methods and accuracies
methods = ['SVM', 'Naive Bayes']
test_accuracies = [accuracy_svm_test, accuracy_nb_test]
train_accuracies = [accuracy_svm_train, accuracy_nb_train]

# Define index for each method
index = np.arange(len(methods))

# Plotting
plt.figure(figsize=(10, 6))

# Plot bars for train accuracies
plt.bar(index, train_accuracies, color='orange', label='Train Accuracy')

# Plot bars for test accuracies on top of train accuracies
plt.bar(index, test_accuracies, color='blue', label='Test Accuracy', bottom=train_accuracies)

# Adding labels and title
plt.xlabel('Method')
plt.ylabel('Accuracy')
plt.title('Comparison of Accuracy between SVM and Naive Bayes')
plt.xticks(index, methods)
plt.legend()

plt.show()

